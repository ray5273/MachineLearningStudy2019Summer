{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2ec47d345b78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# ============================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mDATA_NAME\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Titanic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Digit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mOPTIMIZER\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'SGD'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Momentum'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RMSProp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils import _initialize, optimizer\n",
    "\n",
    "np.random.seed(428)\n",
    "\n",
    "# ========================= EDIT HERE ========================\n",
    "# 1. Choose DATA : Titanic / Digit\n",
    "# 2. Adjust Hyperparameters\n",
    "# 3. Choose Optimizer : SGD / Momentum / RMSProp\n",
    "\n",
    "# DATA\n",
    "DATA_NAME = None\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "batch_size = None\n",
    "num_epochs = None\n",
    "learning_rate = None\n",
    "epsilon = None\n",
    "gamma = None\n",
    "\n",
    "# OPTIMIZER\n",
    "OPTIMIZER = None\n",
    "# ============================================================\n",
    "\n",
    "assert DATA_NAME in ['Titanic', 'Digit']\n",
    "assert OPTIMIZER in ['SGD', 'Momentum', 'RMSProp']\n",
    "\n",
    "# Load dataset, model and evaluation metric\n",
    "train_data, test_data, logistic_regression, metric = _initialize(DATA_NAME)\n",
    "train_x, train_y = train_data\n",
    "\n",
    "num_data, num_features = train_x.shape\n",
    "print('# of Training data : ', num_data)\n",
    "\n",
    "# Make model & optimizer\n",
    "model = logistic_regression(num_features)\n",
    "optim = optimizer(OPTIMIZER, gamma=gamma, epsilon=epsilon)\n",
    "\n",
    "# TRAIN\n",
    "loss = model.train(train_x, train_y, num_epochs, batch_size, learning_rate, optim)\n",
    "print('Training Loss at last epoch: %.2f' % loss)\n",
    "\n",
    "# EVALUATION\n",
    "test_x, test_y = test_data\n",
    "pred = model.eval(test_x)\n",
    "\n",
    "ACC = metric(pred, test_y)\n",
    "\n",
    "print(OPTIMIZER, ' ACC on Test Data : %.3f' % ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
