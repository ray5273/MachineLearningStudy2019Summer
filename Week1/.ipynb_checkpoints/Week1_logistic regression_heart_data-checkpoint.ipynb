{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility --> random 값을 매번 같은 것을 반환함\n",
    "#sklearn 추가\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235, 13)\n",
      "(235, 1)\n",
      "(57, 13)\n",
      "(57, 1)\n",
      "WARNING:tensorflow:From C:\\Users\\sanghyeok\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "(test) hypothesis:  [[0.8650817 ]\n",
      " [0.02060673]\n",
      " [0.02473754]\n",
      " [0.72879195]\n",
      " [0.9183578 ]\n",
      " [0.8664768 ]\n",
      " [0.33775896]\n",
      " [0.718077  ]\n",
      " [0.21057898]\n",
      " [0.07346615]\n",
      " [0.7628635 ]\n",
      " [0.83954906]\n",
      " [0.7596277 ]\n",
      " [0.8851917 ]\n",
      " [0.5046264 ]\n",
      " [0.7243991 ]\n",
      " [0.95276815]\n",
      " [0.84872854]\n",
      " [0.50332594]\n",
      " [0.9734896 ]\n",
      " [0.4146846 ]\n",
      " [0.02477533]\n",
      " [0.853901  ]\n",
      " [0.06429642]\n",
      " [0.9094495 ]\n",
      " [0.13517281]\n",
      " [0.89078593]\n",
      " [0.7951417 ]\n",
      " [0.6105474 ]\n",
      " [0.8291048 ]\n",
      " [0.8971    ]\n",
      " [0.8940474 ]\n",
      " [0.20861235]\n",
      " [0.70848966]\n",
      " [0.92448837]\n",
      " [0.5568888 ]\n",
      " [0.5544909 ]\n",
      " [0.8416747 ]\n",
      " [0.12545985]\n",
      " [0.03256509]\n",
      " [0.567205  ]\n",
      " [0.5370764 ]\n",
      " [0.52032036]\n",
      " [0.5687738 ]\n",
      " [0.01448336]\n",
      " [0.978228  ]\n",
      " [0.20061079]\n",
      " [0.68400556]\n",
      " [0.95075357]\n",
      " [0.3260569 ]\n",
      " [0.0260075 ]\n",
      " [0.6086015 ]\n",
      " [0.6401905 ]\n",
      " [0.93022954]\n",
      " [0.04632321]\n",
      " [0.8666786 ]\n",
      " [0.7397519 ]\n",
      " [0.16946733]\n",
      " [0.7704812 ]\n",
      " [0.8796705 ]\n",
      " [0.25036722]\n",
      " [0.8638315 ]\n",
      " [0.8799261 ]\n",
      " [0.842477  ]\n",
      " [0.87659466]\n",
      " [0.55602103]\n",
      " [0.06672701]\n",
      " [0.23475793]\n",
      " [0.95585525]\n",
      " [0.9585277 ]\n",
      " [0.17990798]\n",
      " [0.06305832]\n",
      " [0.2975822 ]\n",
      " [0.7211881 ]\n",
      " [0.77029216]\n",
      " [0.77160156]\n",
      " [0.92351675]\n",
      " [0.8073876 ]\n",
      " [0.924688  ]\n",
      " [0.21668416]\n",
      " [0.7312012 ]\n",
      " [0.17773238]\n",
      " [0.45903865]\n",
      " [0.05968273]\n",
      " [0.38815227]\n",
      " [0.25755   ]\n",
      " [0.15787515]\n",
      " [0.95242953]\n",
      " [0.24607855]\n",
      " [0.631064  ]\n",
      " [0.8131105 ]\n",
      " [0.91565526]\n",
      " [0.13929725]\n",
      " [0.09392744]\n",
      " [0.15610576]\n",
      " [0.02956983]\n",
      " [0.8255586 ]\n",
      " [0.88998806]\n",
      " [0.03765133]\n",
      " [0.05745614]\n",
      " [0.86346304]\n",
      " [0.92238396]\n",
      " [0.5958279 ]\n",
      " [0.3869691 ]\n",
      " [0.42166004]\n",
      " [0.9481724 ]\n",
      " [0.8558868 ]\n",
      " [0.19846919]\n",
      " [0.10882181]\n",
      " [0.80401355]\n",
      " [0.5664214 ]\n",
      " [0.6933474 ]\n",
      " [0.91528845]\n",
      " [0.24853417]\n",
      " [0.29226285]\n",
      " [0.8957317 ]\n",
      " [0.07179213]\n",
      " [0.9229045 ]\n",
      " [0.8054309 ]\n",
      " [0.94937193]\n",
      " [0.84961736]\n",
      " [0.73250157]\n",
      " [0.8294724 ]\n",
      " [0.05305552]\n",
      " [0.03615773]\n",
      " [0.08427602]\n",
      " [0.25102663]\n",
      " [0.29831204]\n",
      " [0.6684547 ]\n",
      " [0.80103225]\n",
      " [0.10012379]\n",
      " [0.937098  ]\n",
      " [0.7329086 ]\n",
      " [0.83365023]\n",
      " [0.29883727]\n",
      " [0.6937914 ]\n",
      " [0.192574  ]\n",
      " [0.8952472 ]\n",
      " [0.09510767]\n",
      " [0.11165127]\n",
      " [0.49861568]\n",
      " [0.77519554]\n",
      " [0.03539494]\n",
      " [0.27799767]\n",
      " [0.11933973]\n",
      " [0.78681576]\n",
      " [0.6534649 ]\n",
      " [0.40632492]\n",
      " [0.760131  ]\n",
      " [0.37929094]\n",
      " [0.8778373 ]\n",
      " [0.13118735]\n",
      " [0.5415948 ]\n",
      " [0.16576964]\n",
      " [0.9010587 ]\n",
      " [0.01256096]\n",
      " [0.3759727 ]\n",
      " [0.85119   ]\n",
      " [0.06526417]\n",
      " [0.7421093 ]\n",
      " [0.4676185 ]\n",
      " [0.9408725 ]\n",
      " [0.83934945]\n",
      " [0.8890785 ]\n",
      " [0.6478042 ]\n",
      " [0.7868726 ]\n",
      " [0.8986218 ]\n",
      " [0.02172437]\n",
      " [0.02204034]\n",
      " [0.8334374 ]\n",
      " [0.5193386 ]\n",
      " [0.4388914 ]\n",
      " [0.2066949 ]\n",
      " [0.95848   ]\n",
      " [0.47530195]\n",
      " [0.48799288]\n",
      " [0.749222  ]\n",
      " [0.9602346 ]\n",
      " [0.9772465 ]\n",
      " [0.96113396]\n",
      " [0.7360165 ]\n",
      " [0.94593215]\n",
      " [0.09338158]\n",
      " [0.19157648]\n",
      " [0.92844915]\n",
      " [0.24963507]\n",
      " [0.93532574]\n",
      " [0.03654024]\n",
      " [0.0481233 ]\n",
      " [0.44714049]\n",
      " [0.9131221 ]\n",
      " [0.84748995]\n",
      " [0.8390784 ]\n",
      " [0.9464619 ]\n",
      " [0.60894954]\n",
      " [0.9481083 ]\n",
      " [0.3589474 ]\n",
      " [0.50029594]\n",
      " [0.08997616]\n",
      " [0.33732325]\n",
      " [0.8075371 ]\n",
      " [0.15315464]\n",
      " [0.05857739]\n",
      " [0.94696784]\n",
      " [0.7003748 ]\n",
      " [0.9549612 ]\n",
      " [0.75358826]\n",
      " [0.72839785]\n",
      " [0.89330846]\n",
      " [0.97736925]\n",
      " [0.91018796]\n",
      " [0.06398079]\n",
      " [0.3035732 ]\n",
      " [0.8255078 ]\n",
      " [0.64608014]\n",
      " [0.88124645]\n",
      " [0.688631  ]\n",
      " [0.02949464]\n",
      " [0.7900233 ]\n",
      " [0.6743566 ]\n",
      " [0.85221386]\n",
      " [0.93462074]\n",
      " [0.6647825 ]\n",
      " [0.8041365 ]\n",
      " [0.06907246]\n",
      " [0.9172143 ]\n",
      " [0.1523864 ]\n",
      " [0.3177546 ]\n",
      " [0.6534165 ]\n",
      " [0.705659  ]\n",
      " [0.5620466 ]\n",
      " [0.93945104]\n",
      " [0.25110775]\n",
      " [0.03601044]\n",
      " [0.54522777]] \n",
      "Correct (Y): [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]] \n",
      "Accuracy  0.8468085\n",
      "\n",
      "(training) hypothesis:  [[0.9001118 ]\n",
      " [0.10531864]\n",
      " [0.8679406 ]\n",
      " [0.4164197 ]\n",
      " [0.84803414]\n",
      " [0.01596141]\n",
      " [0.3325843 ]\n",
      " [0.61084896]\n",
      " [0.02480909]\n",
      " [0.03644189]\n",
      " [0.77010524]\n",
      " [0.87818515]\n",
      " [0.04369408]\n",
      " [0.38390332]\n",
      " [0.76528466]\n",
      " [0.6874887 ]\n",
      " [0.9238136 ]\n",
      " [0.40310842]\n",
      " [0.5897274 ]\n",
      " [0.73302925]\n",
      " [0.05000991]\n",
      " [0.26353273]\n",
      " [0.06923354]\n",
      " [0.29786322]\n",
      " [0.8449465 ]\n",
      " [0.60252744]\n",
      " [0.79233575]\n",
      " [0.4271813 ]\n",
      " [0.05340275]\n",
      " [0.3373819 ]\n",
      " [0.6456801 ]\n",
      " [0.4452297 ]\n",
      " [0.01547194]\n",
      " [0.82732517]\n",
      " [0.09588033]\n",
      " [0.19530058]\n",
      " [0.06929749]\n",
      " [0.02090821]\n",
      " [0.28277043]\n",
      " [0.4411933 ]\n",
      " [0.9657338 ]\n",
      " [0.6145563 ]\n",
      " [0.10420036]\n",
      " [0.8093521 ]\n",
      " [0.13039696]\n",
      " [0.58020455]\n",
      " [0.8322508 ]\n",
      " [0.23317972]\n",
      " [0.96736336]\n",
      " [0.6377665 ]\n",
      " [0.6718968 ]\n",
      " [0.05524606]\n",
      " [0.8353324 ]\n",
      " [0.9057073 ]\n",
      " [0.44892457]\n",
      " [0.9142791 ]\n",
      " [0.6611105 ]] \n",
      "Correct (Y): [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy  0.7719298\n"
     ]
    }
   ],
   "source": [
    "#여기는 강의안 따라 만들어 본 것\n",
    "tf.set_random_seed(777)  # for reproducibility --> random 값을 매번 같은 것을 반환함\n",
    "xy_train = np.loadtxt('Heart_Train_Changed.csv',skiprows=1,delimiter=',',dtype=np.float32)\n",
    "xy_test = np.loadtxt('Heart_Test_Changed.csv',skiprows=1,delimiter=',',dtype=np.float32)\n",
    "#input data를 normalize 하지 않으면 학습이 제대로 안된다!\n",
    "\n",
    "# 이런식으로 함수를 만들어도 되고 sklearn을 통해서 normalize 함수를 불러와도 되고\n",
    "# def MinMaxScaler(data):\n",
    "#     numerator = data - np.min(data, 0)\n",
    "#     denominator = np.max(data, 0) - np.min(data, 0)\n",
    "#     # noise term prevents the zero division\n",
    "#     return numerator / (denominator + 1e-5)\n",
    "# xy = MinMaxScaler(xy)\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "xy_train= scaler.fit_transform(xy_train)\n",
    "xy_test = scaler.fit_transform(xy_test)\n",
    "\n",
    "\n",
    "x_data_train = xy_train[:,0:-1]\n",
    "y_data_train = xy_train[:,[-1]]\n",
    "x_data_test = xy_test[:,0:-1]\n",
    "y_data_test = xy_test[:,[-1]]\n",
    "\n",
    "# x_data_train = xy[:250,0:-1]\n",
    "# x_data_test = xy[250:,0:-1]\n",
    "# y_data_train = xy[:250,[-1]]\n",
    "# y_data_test = xy[250:,[-1]]\n",
    "\n",
    "\n",
    "# print(x_data_train)\n",
    "print(x_data_train.shape)\n",
    "# print(y_data_train)\n",
    "print(y_data_train.shape)\n",
    "\n",
    "# print(x_data_test)\n",
    "print(x_data_test.shape)\n",
    "# print(y_data_test)\n",
    "print(y_data_test.shape)\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=[None,13])\n",
    "Y = tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([13,1]),name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]),name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W)+b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#predicted 값을 설정하기위해서 hypothesis 값을 어떤식으로 조정해야하지?\n",
    "#어떻게 해야 좋은게\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], \n",
    "                               feed_dict={X: x_data_train, Y: y_data_train})\n",
    "#         if step%100 == 0:\n",
    "#             print(step,cost_val)\n",
    "    \n",
    "    h, c, a = sess.run([hypothesis,predicted, accuracy],\n",
    "                               feed_dict = {X:x_data_train,Y:y_data_train})\n",
    "    print(\"\\n(test) hypothesis: \",h,\"\\nCorrect (Y):\",c,\"\\nAccuracy \",a)\n",
    "    \n",
    "    h, c, a = sess.run([hypothesis,predicted, accuracy],\n",
    "                               feed_dict = {X:x_data_test,Y:y_data_test})\n",
    "    print(\"\\n(training) hypothesis: \",h,\"\\nCorrect (Y):\",c,\"\\nAccuracy \",a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235,)\n",
      "Test Accuracy (training) 86.38%\n",
      "Test Accuracy (test) 80.70%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "\n",
    "# error fix link \n",
    "# :https://www.kaggle.com/pratsiuk/valueerror-unknown-label-type-continuous \n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "\n",
    "#np.ravel 이게 뭔지 잘 모르겠네\n",
    "\n",
    "training_scores_y = lab_enc.fit_transform(np.ravel(y_data_train))\n",
    "print(np.ravel(y_data_train).shape)\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "lr.fit(x_data_train,training_scores_y)\n",
    "print(\"Test Accuracy (training) {:.2f}%\".format(lr.score(x_data_train,y_data_train)*100))\n",
    "print(\"Test Accuracy (test) {:.2f}%\".format(lr.score(x_data_test,y_data_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (training): 0.55\n",
      "Test Accuracy (test): 0.49\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge(alpha=0.01).fit(x_data_train,y_data_train)\n",
    "print(\"Test Accuracy (training): {:.2f}\".format(ridge.score(x_data_train, y_data_train)))\n",
    "print(\"Test Accuracy (test): {:.2f}\".format(ridge.score(x_data_test, y_data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "0 1.4232267\n",
      "100 1.0416833\n",
      "200 0.7986002\n",
      "300 0.6507805\n",
      "400 0.5576831\n",
      "500 0.49587002\n",
      "600 0.453263\n",
      "700 0.42316946\n",
      "800 0.40151027\n",
      "900 0.38564327\n",
      "1000 0.3738093\n",
      "1100 0.36482346\n",
      "1200 0.3578806\n",
      "1300 0.3524273\n",
      "1400 0.34807846\n",
      "1500 0.34456158\n",
      "1600 0.3416811\n",
      "1700 0.3392943\n",
      "1800 0.33729568\n",
      "1900 0.33560613\n",
      "2000 0.33416528\n",
      "2100 0.33292687\n",
      "2200 0.33185473\n",
      "2300 0.33092043\n",
      "2400 0.3301015\n",
      "2500 0.32937962\n",
      "2600 0.3287402\n",
      "2700 0.32817128\n",
      "2800 0.32766297\n",
      "2900 0.3272071\n",
      "3000 0.32679683\n",
      "3100 0.32642645\n",
      "3200 0.3260911\n",
      "3300 0.32578662\n",
      "3400 0.32550955\n",
      "3500 0.3252569\n",
      "3600 0.325026\n",
      "3700 0.3248146\n",
      "3800 0.3246207\n",
      "3900 0.3244426\n",
      "4000 0.32427883\n",
      "4100 0.32412794\n",
      "4200 0.32398883\n",
      "4300 0.32386038\n",
      "4400 0.3237417\n",
      "4500 0.3236319\n",
      "4600 0.32353035\n",
      "4700 0.32343623\n",
      "4800 0.32334894\n",
      "4900 0.32326794\n",
      "5000 0.3231928\n",
      "5100 0.323123\n",
      "5200 0.32305813\n",
      "5300 0.32299784\n",
      "5400 0.32294172\n",
      "5500 0.32288957\n",
      "5600 0.32284096\n",
      "5700 0.3227957\n",
      "5800 0.32275355\n",
      "5900 0.32271427\n",
      "6000 0.32267767\n",
      "6100 0.32264355\n",
      "6200 0.32261175\n",
      "6300 0.32258207\n",
      "6400 0.32255435\n",
      "6500 0.32252857\n",
      "6600 0.32250443\n",
      "6700 0.32248193\n",
      "6800 0.32246092\n",
      "6900 0.3224413\n",
      "7000 0.32242295\n",
      "7100 0.3224059\n",
      "7200 0.3223899\n",
      "7300 0.32237494\n",
      "7400 0.32236105\n",
      "7500 0.322348\n",
      "7600 0.32233578\n",
      "7700 0.3223244\n",
      "7800 0.3223138\n",
      "7900 0.32230383\n",
      "8000 0.3222946\n",
      "8100 0.32228586\n",
      "8200 0.32227772\n",
      "8300 0.3222701\n",
      "8400 0.32226303\n",
      "8500 0.32225642\n",
      "8600 0.32225013\n",
      "8700 0.32224435\n",
      "8800 0.3222389\n",
      "8900 0.32223386\n",
      "9000 0.32222915\n",
      "9100 0.32222462\n",
      "9200 0.3222205\n",
      "9300 0.3222166\n",
      "9400 0.32221296\n",
      "9500 0.32220957\n",
      "9600 0.3222064\n",
      "9700 0.32220343\n",
      "9800 0.32220063\n",
      "9900 0.322198\n",
      "10000 0.3221956\n",
      "\n",
      "(test) hypothesis:  [[9.00205970e-01]\n",
      " [1.52948499e-03]\n",
      " [5.46738505e-03]\n",
      " [6.77354157e-01]\n",
      " [9.74713266e-01]\n",
      " [9.36216354e-01]\n",
      " [1.09837353e-01]\n",
      " [9.05934453e-01]\n",
      " [6.85711801e-02]\n",
      " [2.72146761e-02]\n",
      " [7.82546163e-01]\n",
      " [8.92328501e-01]\n",
      " [8.40541720e-01]\n",
      " [9.19548035e-01]\n",
      " [5.48534811e-01]\n",
      " [6.54839277e-01]\n",
      " [9.88748670e-01]\n",
      " [8.97358656e-01]\n",
      " [8.03525329e-01]\n",
      " [9.93776083e-01]\n",
      " [1.93917394e-01]\n",
      " [4.89491224e-03]\n",
      " [8.85618448e-01]\n",
      " [4.59314287e-02]\n",
      " [9.47804928e-01]\n",
      " [9.00674164e-02]\n",
      " [9.28698778e-01]\n",
      " [8.23907971e-01]\n",
      " [8.01851988e-01]\n",
      " [7.91876197e-01]\n",
      " [9.17631507e-01]\n",
      " [9.57305729e-01]\n",
      " [3.78590226e-02]\n",
      " [7.83075333e-01]\n",
      " [9.87057269e-01]\n",
      " [5.73223174e-01]\n",
      " [5.37116587e-01]\n",
      " [9.25886035e-01]\n",
      " [6.58221841e-02]\n",
      " [7.80594349e-03]\n",
      " [4.74563003e-01]\n",
      " [6.44916058e-01]\n",
      " [5.00107348e-01]\n",
      " [8.31921995e-01]\n",
      " [1.95288658e-03]\n",
      " [9.96389747e-01]\n",
      " [7.08920956e-02]\n",
      " [6.31218016e-01]\n",
      " [9.79337454e-01]\n",
      " [1.68566734e-01]\n",
      " [4.92125750e-03]\n",
      " [6.08128428e-01]\n",
      " [7.45539844e-01]\n",
      " [9.55907226e-01]\n",
      " [1.34089589e-02]\n",
      " [8.99587989e-01]\n",
      " [8.21413279e-01]\n",
      " [1.82539731e-01]\n",
      " [8.57009590e-01]\n",
      " [9.28872943e-01]\n",
      " [1.23650491e-01]\n",
      " [9.23591495e-01]\n",
      " [9.44410563e-01]\n",
      " [9.06340599e-01]\n",
      " [9.48109031e-01]\n",
      " [5.38608551e-01]\n",
      " [2.71517038e-03]\n",
      " [6.68526590e-02]\n",
      " [9.96015787e-01]\n",
      " [9.93146479e-01]\n",
      " [1.61749929e-01]\n",
      " [4.35107052e-02]\n",
      " [8.27156901e-02]\n",
      " [7.08364487e-01]\n",
      " [8.68946195e-01]\n",
      " [8.72840703e-01]\n",
      " [9.85968292e-01]\n",
      " [8.79430532e-01]\n",
      " [9.85555768e-01]\n",
      " [7.12826848e-02]\n",
      " [8.22603345e-01]\n",
      " [1.58873469e-01]\n",
      " [4.17316347e-01]\n",
      " [1.62364841e-02]\n",
      " [2.68802255e-01]\n",
      " [2.64127851e-01]\n",
      " [1.24509245e-01]\n",
      " [9.94337678e-01]\n",
      " [1.81739241e-01]\n",
      " [6.08433664e-01]\n",
      " [8.69419813e-01]\n",
      " [9.63157654e-01]\n",
      " [2.12979019e-02]\n",
      " [5.15057743e-02]\n",
      " [1.06329024e-02]\n",
      " [2.92113423e-03]\n",
      " [8.61482799e-01]\n",
      " [9.65773702e-01]\n",
      " [1.71178579e-03]\n",
      " [2.21185684e-02]\n",
      " [9.42768276e-01]\n",
      " [9.81146216e-01]\n",
      " [6.69550061e-01]\n",
      " [7.16254950e-01]\n",
      " [5.57006836e-01]\n",
      " [9.91974711e-01]\n",
      " [9.26782370e-01]\n",
      " [6.73589706e-02]\n",
      " [7.72756040e-02]\n",
      " [8.50789070e-01]\n",
      " [7.59728312e-01]\n",
      " [6.03293955e-01]\n",
      " [9.71255362e-01]\n",
      " [2.42815673e-01]\n",
      " [5.65732419e-01]\n",
      " [9.70862389e-01]\n",
      " [3.54337692e-03]\n",
      " [9.81439114e-01]\n",
      " [8.67426157e-01]\n",
      " [9.92119789e-01]\n",
      " [8.57818365e-01]\n",
      " [8.72205555e-01]\n",
      " [8.66133213e-01]\n",
      " [1.12446547e-02]\n",
      " [1.10554993e-02]\n",
      " [6.48325682e-03]\n",
      " [1.74440920e-01]\n",
      " [1.43100142e-01]\n",
      " [6.77291512e-01]\n",
      " [9.26434994e-01]\n",
      " [5.55283129e-02]\n",
      " [9.88647640e-01]\n",
      " [8.78864765e-01]\n",
      " [8.88699651e-01]\n",
      " [3.99696052e-01]\n",
      " [8.99869680e-01]\n",
      " [2.05783248e-01]\n",
      " [9.80006456e-01]\n",
      " [1.97348297e-02]\n",
      " [7.09490180e-02]\n",
      " [5.41889608e-01]\n",
      " [7.58817673e-01]\n",
      " [4.96959686e-03]\n",
      " [1.39331579e-01]\n",
      " [6.28148317e-02]\n",
      " [7.36108363e-01]\n",
      " [6.23919785e-01]\n",
      " [3.16062927e-01]\n",
      " [7.32408881e-01]\n",
      " [7.54445791e-02]\n",
      " [9.41116333e-01]\n",
      " [4.37190831e-02]\n",
      " [2.15325832e-01]\n",
      " [3.07025015e-02]\n",
      " [9.65612948e-01]\n",
      " [5.18620014e-04]\n",
      " [1.12382680e-01]\n",
      " [9.47326183e-01]\n",
      " [4.20624018e-02]\n",
      " [7.13879585e-01]\n",
      " [6.99713707e-01]\n",
      " [9.85187888e-01]\n",
      " [8.54906201e-01]\n",
      " [9.73485231e-01]\n",
      " [6.73834205e-01]\n",
      " [8.30668151e-01]\n",
      " [9.51319098e-01]\n",
      " [3.36065888e-03]\n",
      " [3.17263603e-03]\n",
      " [8.91549706e-01]\n",
      " [6.34541631e-01]\n",
      " [5.58942974e-01]\n",
      " [9.12006199e-02]\n",
      " [9.95703220e-01]\n",
      " [7.14818060e-01]\n",
      " [2.65334904e-01]\n",
      " [7.79794216e-01]\n",
      " [9.96004581e-01]\n",
      " [9.98807192e-01]\n",
      " [9.94123638e-01]\n",
      " [8.29023242e-01]\n",
      " [9.79431272e-01]\n",
      " [9.63042080e-02]\n",
      " [2.64510512e-01]\n",
      " [9.85995412e-01]\n",
      " [6.90785348e-02]\n",
      " [9.72249389e-01]\n",
      " [6.42937422e-03]\n",
      " [1.61962211e-02]\n",
      " [3.58688444e-01]\n",
      " [9.71003771e-01]\n",
      " [9.13526773e-01]\n",
      " [9.31686282e-01]\n",
      " [9.86107647e-01]\n",
      " [4.42026496e-01]\n",
      " [9.91233528e-01]\n",
      " [1.99989855e-01]\n",
      " [5.38856626e-01]\n",
      " [6.24386966e-02]\n",
      " [1.61818624e-01]\n",
      " [8.92236829e-01]\n",
      " [2.10212171e-02]\n",
      " [2.47495472e-02]\n",
      " [9.89635706e-01]\n",
      " [7.12475657e-01]\n",
      " [9.94282484e-01]\n",
      " [8.63076329e-01]\n",
      " [6.93704724e-01]\n",
      " [9.68515098e-01]\n",
      " [9.95506525e-01]\n",
      " [9.73324180e-01]\n",
      " [1.74323320e-02]\n",
      " [2.11775690e-01]\n",
      " [8.43201876e-01]\n",
      " [5.46653807e-01]\n",
      " [9.51829195e-01]\n",
      " [4.59505290e-01]\n",
      " [3.51330638e-03]\n",
      " [9.07286406e-01]\n",
      " [5.94026685e-01]\n",
      " [9.20997024e-01]\n",
      " [9.83344913e-01]\n",
      " [5.79820395e-01]\n",
      " [8.89802635e-01]\n",
      " [2.29782462e-02]\n",
      " [9.70270038e-01]\n",
      " [1.34439439e-01]\n",
      " [1.63445354e-01]\n",
      " [7.18828440e-01]\n",
      " [8.12940359e-01]\n",
      " [4.81487513e-01]\n",
      " [9.74213541e-01]\n",
      " [2.63813496e-01]\n",
      " [5.15655009e-03]\n",
      " [8.37473273e-01]] \n",
      "Correct (Y): [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]] \n",
      "Accuracy  0.87234044\n",
      "\n",
      "(training) hypothesis:  [[0.9804617 ]\n",
      " [0.04511902]\n",
      " [0.9717712 ]\n",
      " [0.44083124]\n",
      " [0.9586256 ]\n",
      " [0.00349975]\n",
      " [0.63920623]\n",
      " [0.76677406]\n",
      " [0.00824443]\n",
      " [0.01987159]\n",
      " [0.812763  ]\n",
      " [0.96677077]\n",
      " [0.04237342]\n",
      " [0.47455615]\n",
      " [0.88086313]\n",
      " [0.8091053 ]\n",
      " [0.9905863 ]\n",
      " [0.35711455]\n",
      " [0.47280964]\n",
      " [0.90311086]\n",
      " [0.0297251 ]\n",
      " [0.5299128 ]\n",
      " [0.05180225]\n",
      " [0.55373144]\n",
      " [0.9536724 ]\n",
      " [0.81021774]\n",
      " [0.9158739 ]\n",
      " [0.63973415]\n",
      " [0.03646982]\n",
      " [0.3848468 ]\n",
      " [0.8543315 ]\n",
      " [0.86198556]\n",
      " [0.00529012]\n",
      " [0.958963  ]\n",
      " [0.01532081]\n",
      " [0.05148432]\n",
      " [0.05865386]\n",
      " [0.00326753]\n",
      " [0.59499633]\n",
      " [0.85207736]\n",
      " [0.99790573]\n",
      " [0.6692339 ]\n",
      " [0.0725199 ]\n",
      " [0.9552153 ]\n",
      " [0.04531506]\n",
      " [0.6843513 ]\n",
      " [0.92461574]\n",
      " [0.08527625]\n",
      " [0.997589  ]\n",
      " [0.79652333]\n",
      " [0.93112385]\n",
      " [0.04981068]\n",
      " [0.99288625]\n",
      " [0.9719105 ]\n",
      " [0.73814654]\n",
      " [0.9887825 ]\n",
      " [0.60738224]] \n",
      "Correct (Y): [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy  0.8245614\n"
     ]
    }
   ],
   "source": [
    "#StandardScaler\n",
    "xy_train = np.loadtxt('Heart_Train_Changed.csv',skiprows=1,delimiter=',',dtype=np.float32)\n",
    "xy_test = np.loadtxt('Heart_Test_Changed.csv',skiprows=1,delimiter=',',dtype=np.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_data_train = xy_train[:,0:-1]\n",
    "y_data_train = xy_train[:,[-1]]\n",
    "\n",
    "x_data_test = xy_test[:,0:-1]\n",
    "y_data_test = xy_test[:,[-1]]\n",
    "# xy_train= scaler.fit_transform(xy_train)\n",
    "# xy_test = scaler.fit_transform(xy_test)\n",
    "\n",
    "x_data_train = scaler.fit_transform(x_data_train)\n",
    "x_data_test = scaler.fit_transform(x_data_test)\n",
    "\n",
    "# print(y_data_train)\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=[None,13])\n",
    "Y = tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([13,1]),name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]),name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W)+b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#predicted 값을 설정하기위해서 hypothesis 값을 어떤식으로 조정해야하지?\n",
    "#어떻게 해야 좋은게\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], \n",
    "                               feed_dict={X: x_data_train, Y: y_data_train})\n",
    "        if step%100 == 0:\n",
    "            print(step,cost_val)\n",
    "    \n",
    "    h, c, a = sess.run([hypothesis,predicted, accuracy],\n",
    "                               feed_dict = {X:x_data_train,Y:y_data_train})\n",
    "    print(\"\\n(test) hypothesis: \",h,\"\\nCorrect (Y):\",c,\"\\nAccuracy \",a)\n",
    "    \n",
    "    h, c, a = sess.run([hypothesis,predicted, accuracy],\n",
    "                               feed_dict = {X:x_data_test,Y:y_data_test})\n",
    "    print(\"\\n(training) hypothesis: \",h,\"\\nCorrect (Y):\",c,\"\\nAccuracy \",a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "(235, 13)\n",
      "Tensor(\"Softmax_18:0\", shape=(?, 2), dtype=float32)\n",
      "0 0.9725322\n",
      "200 0.6540695\n",
      "400 0.6078578\n",
      "600 0.57304096\n",
      "800 0.54424626\n",
      "1000 0.5201676\n",
      "1200 0.49985313\n",
      "1400 0.4825743\n",
      "1600 0.46776655\n",
      "1800 0.45498845\n",
      "2000 0.4438917\n",
      "2200 0.4341989\n",
      "2400 0.4256873\n",
      "2600 0.41817665\n",
      "2800 0.41151947\n",
      "3000 0.40559426\n",
      "3200 0.40030015\n",
      "3400 0.39555278\n",
      "3600 0.391281\n",
      "3800 0.38742462\n",
      "4000 0.3839323\n",
      "4200 0.38076007\n",
      "4400 0.37787017\n",
      "4600 0.3752299\n",
      "4800 0.37281102\n",
      "5000 0.37058872\n",
      "5200 0.36854178\n",
      "5400 0.36665127\n",
      "5600 0.36490086\n",
      "5800 0.36327595\n",
      "6000 0.361764\n",
      "6200 0.3603538\n",
      "6400 0.3590354\n",
      "6600 0.35780004\n",
      "6800 0.35664\n",
      "7000 0.35554844\n",
      "7200 0.3545192\n",
      "7400 0.35354686\n",
      "7600 0.3526266\n",
      "7800 0.35175395\n",
      "8000 0.35092518\n",
      "8200 0.35013682\n",
      "8400 0.34938562\n",
      "8600 0.3486689\n",
      "8800 0.34798408\n",
      "9000 0.34732896\n",
      "9200 0.34670138\n",
      "9400 0.34609947\n",
      "9600 0.3455216\n",
      "9800 0.34496623\n",
      "10000 0.3444319\n",
      "Accuracy:  0.7894737\n"
     ]
    }
   ],
   "source": [
    "#여기는 강의안 따라 만들어 본 것 + Softmax\n",
    "tf.set_random_seed(777)  # for reproducibility --> random 값을 매번 같은 것을 반환함\n",
    "xy_train = np.loadtxt('Heart_Train_Changed.csv',skiprows=1,delimiter=',',dtype=np.float32)\n",
    "xy_test = np.loadtxt('Heart_Test_Changed.csv',skiprows=1,delimiter=',',dtype=np.float32)\n",
    "#input data를 normalize 하지 않으면 학습이 제대로 안된다!\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "xy_train= scaler.fit_transform(xy_train)\n",
    "xy_test = scaler.fit_transform(xy_test)\n",
    "\n",
    "\n",
    "x_data_train = xy_train[:,0:-1]\n",
    "y_data_train = xy_train[:,[-1]]\n",
    "x_data_test = xy_test[:,0:-1]\n",
    "y_data_test = xy_test[:,[-1]]\n",
    "\n",
    "\n",
    "\n",
    "#0,1 data를 one hot 방식으로 두가지 클래스로 나눠서 y값에 다시 넣어줌 (softmax 를 위해서)\n",
    "y_data_modified_for_softmax = []\n",
    "for i in range(len(y_data_train)):\n",
    "    if y_data_train[i]==0: \n",
    "        y_data_modified_for_softmax.append([1,0])\n",
    "    else:\n",
    "        y_data_modified_for_softmax.append([0,1])\n",
    "        \n",
    "y_data_modified_for_softmax_test = []\n",
    "for i in range(len(y_data_test)):\n",
    "    if y_data_test[i]==0: \n",
    "        y_data_modified_for_softmax_test.append([1,0])\n",
    "    else:\n",
    "        y_data_modified_for_softmax_test.append([0,1])        \n",
    "\n",
    "for i in range(len(y_data_train)):\n",
    "        print(y_data_train[i],y_data_modified_for_softmax[i])\n",
    "print(x_data_train.shape)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=[None,13])\n",
    "Y = tf.placeholder(tf.float32,shape=[None,2])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([13,2]),name='weight')\n",
    "b = tf.Variable(tf.random_normal([2]),name='bias')\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X,W)+b)\n",
    "\n",
    "print(hypothesis)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis),axis=1))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypothesis,1),tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        _,cost_val = sess.run([train,cost], \n",
    "                               feed_dict={X: x_data_train, Y: y_data_modified_for_softmax})\n",
    "        if step%200 == 0:\n",
    "             print(step,cost_val)\n",
    "    print(\n",
    "        \"Accuracy: \",\n",
    "        accuracy.eval(\n",
    "            session=sess, feed_dict={X: x_data_test, Y: y_data_modified_for_softmax_test}\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
