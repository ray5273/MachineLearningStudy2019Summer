{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility --> random 값을 매번 같은 것을 반환함\n",
    "#sklearn 추가\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235, 13)\n",
      "(235, 1)\n",
      "(57, 13)\n",
      "(57, 1)\n",
      "WARNING:tensorflow:From C:\\Users\\sanghyeok\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "(test) hypothesis:  [[0.8650817 ]\n",
      " [0.02060673]\n",
      " [0.02473754]\n",
      " [0.72879195]\n",
      " [0.9183578 ]\n",
      " [0.8664768 ]\n",
      " [0.33775896]\n",
      " [0.718077  ]\n",
      " [0.21057898]\n",
      " [0.07346615]\n",
      " [0.7628635 ]\n",
      " [0.83954906]\n",
      " [0.7596277 ]\n",
      " [0.8851917 ]\n",
      " [0.5046264 ]\n",
      " [0.7243991 ]\n",
      " [0.95276815]\n",
      " [0.84872854]\n",
      " [0.50332594]\n",
      " [0.9734896 ]\n",
      " [0.4146846 ]\n",
      " [0.02477533]\n",
      " [0.853901  ]\n",
      " [0.06429642]\n",
      " [0.9094495 ]\n",
      " [0.13517281]\n",
      " [0.89078593]\n",
      " [0.7951417 ]\n",
      " [0.6105474 ]\n",
      " [0.8291048 ]\n",
      " [0.8971    ]\n",
      " [0.8940474 ]\n",
      " [0.20861235]\n",
      " [0.70848966]\n",
      " [0.92448837]\n",
      " [0.5568888 ]\n",
      " [0.5544909 ]\n",
      " [0.8416747 ]\n",
      " [0.12545985]\n",
      " [0.03256509]\n",
      " [0.567205  ]\n",
      " [0.5370764 ]\n",
      " [0.52032036]\n",
      " [0.5687738 ]\n",
      " [0.01448336]\n",
      " [0.978228  ]\n",
      " [0.20061079]\n",
      " [0.68400556]\n",
      " [0.95075357]\n",
      " [0.3260569 ]\n",
      " [0.0260075 ]\n",
      " [0.6086015 ]\n",
      " [0.6401905 ]\n",
      " [0.93022954]\n",
      " [0.04632321]\n",
      " [0.8666786 ]\n",
      " [0.7397519 ]\n",
      " [0.16946733]\n",
      " [0.7704812 ]\n",
      " [0.8796705 ]\n",
      " [0.25036722]\n",
      " [0.8638315 ]\n",
      " [0.8799261 ]\n",
      " [0.842477  ]\n",
      " [0.87659466]\n",
      " [0.55602103]\n",
      " [0.06672701]\n",
      " [0.23475793]\n",
      " [0.95585525]\n",
      " [0.9585277 ]\n",
      " [0.17990798]\n",
      " [0.06305832]\n",
      " [0.2975822 ]\n",
      " [0.7211881 ]\n",
      " [0.77029216]\n",
      " [0.77160156]\n",
      " [0.92351675]\n",
      " [0.8073876 ]\n",
      " [0.924688  ]\n",
      " [0.21668416]\n",
      " [0.7312012 ]\n",
      " [0.17773238]\n",
      " [0.45903865]\n",
      " [0.05968273]\n",
      " [0.38815227]\n",
      " [0.25755   ]\n",
      " [0.15787515]\n",
      " [0.95242953]\n",
      " [0.24607855]\n",
      " [0.631064  ]\n",
      " [0.8131105 ]\n",
      " [0.91565526]\n",
      " [0.13929725]\n",
      " [0.09392744]\n",
      " [0.15610576]\n",
      " [0.02956983]\n",
      " [0.8255586 ]\n",
      " [0.88998806]\n",
      " [0.03765133]\n",
      " [0.05745614]\n",
      " [0.86346304]\n",
      " [0.92238396]\n",
      " [0.5958279 ]\n",
      " [0.3869691 ]\n",
      " [0.42166004]\n",
      " [0.9481724 ]\n",
      " [0.8558868 ]\n",
      " [0.19846919]\n",
      " [0.10882181]\n",
      " [0.80401355]\n",
      " [0.5664214 ]\n",
      " [0.6933474 ]\n",
      " [0.91528845]\n",
      " [0.24853417]\n",
      " [0.29226285]\n",
      " [0.8957317 ]\n",
      " [0.07179213]\n",
      " [0.9229045 ]\n",
      " [0.8054309 ]\n",
      " [0.94937193]\n",
      " [0.84961736]\n",
      " [0.73250157]\n",
      " [0.8294724 ]\n",
      " [0.05305552]\n",
      " [0.03615773]\n",
      " [0.08427602]\n",
      " [0.25102663]\n",
      " [0.29831204]\n",
      " [0.6684547 ]\n",
      " [0.80103225]\n",
      " [0.10012379]\n",
      " [0.937098  ]\n",
      " [0.7329086 ]\n",
      " [0.83365023]\n",
      " [0.29883727]\n",
      " [0.6937914 ]\n",
      " [0.192574  ]\n",
      " [0.8952472 ]\n",
      " [0.09510767]\n",
      " [0.11165127]\n",
      " [0.49861568]\n",
      " [0.77519554]\n",
      " [0.03539494]\n",
      " [0.27799767]\n",
      " [0.11933973]\n",
      " [0.78681576]\n",
      " [0.6534649 ]\n",
      " [0.40632492]\n",
      " [0.760131  ]\n",
      " [0.37929094]\n",
      " [0.8778373 ]\n",
      " [0.13118735]\n",
      " [0.5415948 ]\n",
      " [0.16576964]\n",
      " [0.9010587 ]\n",
      " [0.01256096]\n",
      " [0.3759727 ]\n",
      " [0.85119   ]\n",
      " [0.06526417]\n",
      " [0.7421093 ]\n",
      " [0.4676185 ]\n",
      " [0.9408725 ]\n",
      " [0.83934945]\n",
      " [0.8890785 ]\n",
      " [0.6478042 ]\n",
      " [0.7868726 ]\n",
      " [0.8986218 ]\n",
      " [0.02172437]\n",
      " [0.02204034]\n",
      " [0.8334374 ]\n",
      " [0.5193386 ]\n",
      " [0.4388914 ]\n",
      " [0.2066949 ]\n",
      " [0.95848   ]\n",
      " [0.47530195]\n",
      " [0.48799288]\n",
      " [0.749222  ]\n",
      " [0.9602346 ]\n",
      " [0.9772465 ]\n",
      " [0.96113396]\n",
      " [0.7360165 ]\n",
      " [0.94593215]\n",
      " [0.09338158]\n",
      " [0.19157648]\n",
      " [0.92844915]\n",
      " [0.24963507]\n",
      " [0.93532574]\n",
      " [0.03654024]\n",
      " [0.0481233 ]\n",
      " [0.44714049]\n",
      " [0.9131221 ]\n",
      " [0.84748995]\n",
      " [0.8390784 ]\n",
      " [0.9464619 ]\n",
      " [0.60894954]\n",
      " [0.9481083 ]\n",
      " [0.3589474 ]\n",
      " [0.50029594]\n",
      " [0.08997616]\n",
      " [0.33732325]\n",
      " [0.8075371 ]\n",
      " [0.15315464]\n",
      " [0.05857739]\n",
      " [0.94696784]\n",
      " [0.7003748 ]\n",
      " [0.9549612 ]\n",
      " [0.75358826]\n",
      " [0.72839785]\n",
      " [0.89330846]\n",
      " [0.97736925]\n",
      " [0.91018796]\n",
      " [0.06398079]\n",
      " [0.3035732 ]\n",
      " [0.8255078 ]\n",
      " [0.64608014]\n",
      " [0.88124645]\n",
      " [0.688631  ]\n",
      " [0.02949464]\n",
      " [0.7900233 ]\n",
      " [0.6743566 ]\n",
      " [0.85221386]\n",
      " [0.93462074]\n",
      " [0.6647825 ]\n",
      " [0.8041365 ]\n",
      " [0.06907246]\n",
      " [0.9172143 ]\n",
      " [0.1523864 ]\n",
      " [0.3177546 ]\n",
      " [0.6534165 ]\n",
      " [0.705659  ]\n",
      " [0.5620466 ]\n",
      " [0.93945104]\n",
      " [0.25110775]\n",
      " [0.03601044]\n",
      " [0.54522777]] \n",
      "Correct (Y): [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]] \n",
      "Accuracy  0.8468085\n",
      "\n",
      "(training) hypothesis:  [[0.9001118 ]\n",
      " [0.10531864]\n",
      " [0.8679406 ]\n",
      " [0.4164197 ]\n",
      " [0.84803414]\n",
      " [0.01596141]\n",
      " [0.3325843 ]\n",
      " [0.61084896]\n",
      " [0.02480909]\n",
      " [0.03644189]\n",
      " [0.77010524]\n",
      " [0.87818515]\n",
      " [0.04369408]\n",
      " [0.38390332]\n",
      " [0.76528466]\n",
      " [0.6874887 ]\n",
      " [0.9238136 ]\n",
      " [0.40310842]\n",
      " [0.5897274 ]\n",
      " [0.73302925]\n",
      " [0.05000991]\n",
      " [0.26353273]\n",
      " [0.06923354]\n",
      " [0.29786322]\n",
      " [0.8449465 ]\n",
      " [0.60252744]\n",
      " [0.79233575]\n",
      " [0.4271813 ]\n",
      " [0.05340275]\n",
      " [0.3373819 ]\n",
      " [0.6456801 ]\n",
      " [0.4452297 ]\n",
      " [0.01547194]\n",
      " [0.82732517]\n",
      " [0.09588033]\n",
      " [0.19530058]\n",
      " [0.06929749]\n",
      " [0.02090821]\n",
      " [0.28277043]\n",
      " [0.4411933 ]\n",
      " [0.9657338 ]\n",
      " [0.6145563 ]\n",
      " [0.10420036]\n",
      " [0.8093521 ]\n",
      " [0.13039696]\n",
      " [0.58020455]\n",
      " [0.8322508 ]\n",
      " [0.23317972]\n",
      " [0.96736336]\n",
      " [0.6377665 ]\n",
      " [0.6718968 ]\n",
      " [0.05524606]\n",
      " [0.8353324 ]\n",
      " [0.9057073 ]\n",
      " [0.44892457]\n",
      " [0.9142791 ]\n",
      " [0.6611105 ]] \n",
      "Correct (Y): [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy  0.7719298\n"
     ]
    }
   ],
   "source": [
    "#여기는 강의안 따라 만들어 본 것\n",
    "tf.set_random_seed(777)  # for reproducibility --> random 값을 매번 같은 것을 반환함\n",
    "xy_train = np.loadtxt('Heart_Train_Changed.csv',skiprows=1,delimiter=',',dtype=np.float32)\n",
    "xy_test = np.loadtxt('Heart_Test_Changed.csv',skiprows=1,delimiter=',',dtype=np.float32)\n",
    "#input data를 normalize 하지 않으면 학습이 제대로 안된다!\n",
    "\n",
    "# 이런식으로 함수를 만들어도 되고 sklearn을 통해서 normalize 함수를 불러와도 되고\n",
    "# def MinMaxScaler(data):\n",
    "#     numerator = data - np.min(data, 0)\n",
    "#     denominator = np.max(data, 0) - np.min(data, 0)\n",
    "#     # noise term prevents the zero division\n",
    "#     return numerator / (denominator + 1e-5)\n",
    "# xy = MinMaxScaler(xy)\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "xy_train= scaler.fit_transform(xy_train)\n",
    "xy_test = scaler.fit_transform(xy_test)\n",
    "\n",
    "\n",
    "x_data_train = xy_train[:,0:-1]\n",
    "y_data_train = xy_train[:,[-1]]\n",
    "x_data_test = xy_test[:,0:-1]\n",
    "y_data_test = xy_test[:,[-1]]\n",
    "\n",
    "# x_data_train = xy[:250,0:-1]\n",
    "# x_data_test = xy[250:,0:-1]\n",
    "# y_data_train = xy[:250,[-1]]\n",
    "# y_data_test = xy[250:,[-1]]\n",
    "\n",
    "\n",
    "# print(x_data_train)\n",
    "print(x_data_train.shape)\n",
    "# print(y_data_train)\n",
    "print(y_data_train.shape)\n",
    "\n",
    "# print(x_data_test)\n",
    "print(x_data_test.shape)\n",
    "# print(y_data_test)\n",
    "print(y_data_test.shape)\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=[None,13])\n",
    "Y = tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([13,1]),name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]),name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W)+b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#predicted 값을 설정하기위해서 hypothesis 값을 어떤식으로 조정해야하지?\n",
    "#어떻게 해야 좋은게\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], \n",
    "                               feed_dict={X: x_data_train, Y: y_data_train})\n",
    "#         if step%100 == 0:\n",
    "#             print(step,cost_val)\n",
    "    \n",
    "    h, c, a = sess.run([hypothesis,predicted, accuracy],\n",
    "                               feed_dict = {X:x_data_train,Y:y_data_train})\n",
    "    print(\"\\n(test) hypothesis: \",h,\"\\nCorrect (Y):\",c,\"\\nAccuracy \",a)\n",
    "    \n",
    "    h, c, a = sess.run([hypothesis,predicted, accuracy],\n",
    "                               feed_dict = {X:x_data_test,Y:y_data_test})\n",
    "    print(\"\\n(training) hypothesis: \",h,\"\\nCorrect (Y):\",c,\"\\nAccuracy \",a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235,)\n",
      "Test Accuracy (training) 86.38%\n",
      "Test Accuracy (test) 80.70%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "\n",
    "# error fix link \n",
    "# :https://www.kaggle.com/pratsiuk/valueerror-unknown-label-type-continuous \n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "\n",
    "#np.ravel 이게 뭔지 잘 모르겠네\n",
    "\n",
    "training_scores_y = lab_enc.fit_transform(np.ravel(y_data_train))\n",
    "print(np.ravel(y_data_train).shape)\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "lr.fit(x_data_train,training_scores_y)\n",
    "print(\"Test Accuracy (training) {:.2f}%\".format(lr.score(x_data_train,y_data_train)*100))\n",
    "print(\"Test Accuracy (test) {:.2f}%\".format(lr.score(x_data_test,y_data_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (training): 0.55\n",
      "Test Accuracy (test): 0.49\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge(alpha=0.01).fit(x_data_train,y_data_train)\n",
    "print(\"Test Accuracy (training): {:.2f}\".format(ridge.score(x_data_train, y_data_train)))\n",
    "print(\"Test Accuracy (test): {:.2f}\".format(ridge.score(x_data_test, y_data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.93107295  0.7116251   1.982532   ...  2.2572377  -0.71685785\n",
      "  -2.336035  ]\n",
      " [ 1.3735443   0.7116251  -0.92325824 ...  0.64492506  2.5021536\n",
      "  -0.57086384]\n",
      " [ 1.3735443   0.7116251  -0.92325824 ...  0.64492506  1.4291497\n",
      "   1.1943073 ]\n",
      " ...\n",
      " [-1.1706665   0.7116251  -0.92325824 ...  2.2572377  -0.71685785\n",
      "  -2.336035  ]\n",
      " [ 0.93107295  0.7116251  -0.92325824 ... -0.9673876   1.4291497\n",
      "   1.1943073 ]\n",
      " [ 0.93107295 -1.4052343  -0.92325824 ...  0.64492506 -0.71685785\n",
      "  -0.57086384]]\n",
      "0 -0.16046907\n",
      "100 -1.1345983\n",
      "200 -1.8812835\n",
      "300 nan\n",
      "400 nan\n",
      "500 nan\n",
      "600 nan\n",
      "700 nan\n",
      "800 nan\n",
      "900 nan\n",
      "1000 nan\n",
      "1100 nan\n",
      "1200 nan\n",
      "1300 nan\n",
      "1400 nan\n",
      "1500 nan\n",
      "1600 nan\n",
      "1700 nan\n",
      "1800 nan\n",
      "1900 nan\n",
      "2000 nan\n",
      "2100 nan\n",
      "2200 nan\n",
      "2300 nan\n",
      "2400 nan\n",
      "2500 nan\n",
      "2600 nan\n",
      "2700 nan\n",
      "2800 nan\n",
      "2900 nan\n",
      "3000 nan\n",
      "3100 nan\n",
      "3200 nan\n",
      "3300 nan\n",
      "3400 nan\n",
      "3500 nan\n",
      "3600 nan\n",
      "3700 nan\n",
      "3800 nan\n",
      "3900 nan\n",
      "4000 nan\n",
      "4100 nan\n",
      "4200 nan\n",
      "4300 nan\n",
      "4400 nan\n",
      "4500 nan\n",
      "4600 nan\n",
      "4700 nan\n",
      "4800 nan\n",
      "4900 nan\n",
      "5000 nan\n",
      "5100 nan\n",
      "5200 nan\n",
      "5300 nan\n",
      "5400 nan\n",
      "5500 nan\n",
      "5600 nan\n",
      "5700 nan\n",
      "5800 nan\n",
      "5900 nan\n",
      "6000 nan\n",
      "6100 nan\n",
      "6200 nan\n",
      "6300 nan\n",
      "6400 nan\n",
      "6500 nan\n",
      "6600 nan\n",
      "6700 nan\n",
      "6800 nan\n",
      "6900 nan\n",
      "7000 nan\n",
      "7100 nan\n",
      "7200 nan\n",
      "7300 nan\n",
      "7400 nan\n",
      "7500 nan\n",
      "7600 nan\n",
      "7700 nan\n",
      "7800 nan\n",
      "7900 nan\n",
      "8000 nan\n",
      "8100 nan\n",
      "8200 nan\n",
      "8300 nan\n",
      "8400 nan\n",
      "8500 nan\n",
      "8600 nan\n",
      "8700 nan\n",
      "8800 nan\n",
      "8900 nan\n",
      "9000 nan\n",
      "9100 nan\n",
      "9200 nan\n",
      "9300 nan\n",
      "9400 nan\n",
      "9500 nan\n",
      "9600 nan\n",
      "9700 nan\n",
      "9800 nan\n",
      "9900 nan\n",
      "10000 nan\n",
      "\n",
      "(test) hypothesis:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]] \n",
      "Correct (Y): [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy  0.0\n",
      "\n",
      "(training) hypothesis:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]] \n",
      "Correct (Y): [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy  0.0\n"
     ]
    }
   ],
   "source": [
    "#StandardScaler\n",
    "xy_train = np.loadtxt('Heart_Train_Changed.csv',skiprows=1,delimiter=',',dtype=np.float32)\n",
    "xy_test = np.loadtxt('Heart_Test_Changed.csv',skiprows=1,delimiter=',',dtype=np.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "xy_train= scaler.fit_transform(xy_train)\n",
    "xy_test = scaler.fit_transform(xy_test)\n",
    "\n",
    "x_data_train = xy_train[:,0:-1]\n",
    "y_data_train = xy_train[:,[-1]]\n",
    "x_data_test = xy_test[:,0:-1]\n",
    "y_data_test = xy_test[:,[-1]]\n",
    "\n",
    "print(x_data_train)\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=[None,13])\n",
    "Y = tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([13,1]),name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]),name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W)+b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#predicted 값을 설정하기위해서 hypothesis 값을 어떤식으로 조정해야하지?\n",
    "#어떻게 해야 좋은게\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], \n",
    "                               feed_dict={X: x_data_train, Y: y_data_train})\n",
    "        if step%100 == 0:\n",
    "            print(step,cost_val)\n",
    "    \n",
    "    h, c, a = sess.run([hypothesis,predicted, accuracy],\n",
    "                               feed_dict = {X:x_data_train,Y:y_data_train})\n",
    "    print(\"\\n(test) hypothesis: \",h,\"\\nCorrect (Y):\",c,\"\\nAccuracy \",a)\n",
    "    \n",
    "    h, c, a = sess.run([hypothesis,predicted, accuracy],\n",
    "                               feed_dict = {X:x_data_test,Y:y_data_test})\n",
    "    print(\"\\n(training) hypothesis: \",h,\"\\nCorrect (Y):\",c,\"\\nAccuracy \",a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[1.] [0, 1]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "[0.] [1, 0]\n",
      "(235, 13)\n",
      "Tensor(\"Softmax_17:0\", shape=(?, 2), dtype=float32)\n",
      "0 2.0422716\n",
      "200 1.0112292\n",
      "400 0.82155275\n",
      "600 0.6991834\n",
      "800 0.6170467\n",
      "1000 0.5606835\n",
      "1200 0.520709\n",
      "1400 0.49135202\n",
      "1600 0.46908146\n",
      "1800 0.45169744\n",
      "2000 0.43779135\n",
      "2200 0.4264338\n",
      "2400 0.41699317\n",
      "2600 0.4090278\n",
      "2800 0.40222085\n",
      "3000 0.39633957\n",
      "3200 0.3912095\n",
      "3400 0.38669717\n",
      "3600 0.38269913\n",
      "3800 0.37913352\n",
      "4000 0.37593487\n",
      "4200 0.37305045\n",
      "4400 0.3704369\n",
      "4600 0.36805847\n",
      "4800 0.36588544\n",
      "5000 0.3638927\n",
      "5200 0.36205906\n",
      "5400 0.36036658\n",
      "5600 0.35879976\n",
      "5800 0.35734534\n",
      "6000 0.3559917\n",
      "6200 0.35472885\n",
      "6400 0.35354802\n",
      "6600 0.35244143\n",
      "6800 0.35140243\n",
      "7000 0.35042495\n",
      "7200 0.34950364\n",
      "7400 0.34863383\n",
      "7600 0.34781122\n",
      "7800 0.3470321\n",
      "8000 0.34629303\n",
      "8200 0.345591\n",
      "8400 0.34492323\n",
      "8600 0.34428722\n",
      "8800 0.34368074\n",
      "9000 0.34310174\n",
      "9200 0.34254828\n",
      "9400 0.34201884\n",
      "9600 0.34151167\n",
      "9800 0.34102547\n",
      "10000 0.3405589\n"
     ]
    }
   ],
   "source": [
    "#여기는 강의안 따라 만들어 본 것 + Softmax\n",
    "tf.set_random_seed(777)  # for reproducibility --> random 값을 매번 같은 것을 반환함\n",
    "xy_train = np.loadtxt('Heart_Train_Changed.csv',skiprows=1,delimiter=',',dtype=np.float32)\n",
    "xy_test = np.loadtxt('Heart_Test_Changed.csv',skiprows=1,delimiter=',',dtype=np.float32)\n",
    "#input data를 normalize 하지 않으면 학습이 제대로 안된다!\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "xy_train= scaler.fit_transform(xy_train)\n",
    "xy_test = scaler.fit_transform(xy_test)\n",
    "\n",
    "\n",
    "x_data_train = xy_train[:,0:-1]\n",
    "y_data_train = xy_train[:,[-1]]\n",
    "x_data_test = xy_test[:,0:-1]\n",
    "y_data_test = xy_test[:,[-1]]\n",
    "\n",
    "y_data_modified_for_softmax = []\n",
    "for i in range(len(y_data_train)):\n",
    "    if y_data_train[i]==0: \n",
    "        y_data_modified_for_softmax.append([1,0])\n",
    "    else:\n",
    "        y_data_modified_for_softmax.append([0,1])\n",
    "        \n",
    "y_data_modified_for_softmax_test = []\n",
    "for i in range(len(y_data_test)):\n",
    "    if y_data_test[i]==0: \n",
    "        y_data_modified_for_softmax_test.append([1,0])\n",
    "    else:\n",
    "        y_data_modified_for_softmax_test.append([0,1])        \n",
    "\n",
    "for i in range(len(y_data_train)):\n",
    "        print(y_data_train[i],y_data_modified_for_softmax[i])\n",
    "print(x_data_train.shape)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=[None,13])\n",
    "Y = tf.placeholder(tf.float32,shape=[None,2])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([13,2]),name='weight')\n",
    "b = tf.Variable(tf.random_normal([2]),name='bias')\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X,W)+b)\n",
    "\n",
    "print(hypothesis)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis),axis=1))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypothesis,1),tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        _,cost_val = sess.run([train,cost], \n",
    "                               feed_dict={X: x_data_train, Y: y_data_modified_for_softmax})\n",
    "        if step%200 == 0:\n",
    "             print(step,cost_val)\n",
    "    print(\n",
    "        \"Accuracy: \",\n",
    "        accuracy.eval(\n",
    "            session=sess, feed_dict={X: x_data_test, Y: y_data_modified_for_softmax_test}\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
